{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073e856e-77df-433e-9a91-143341213727",
   "metadata": {},
   "source": [
    "### Импорт утилит и настройка путей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e4103-47fe-4961-b269-648c698c460c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file created successfully: F:\\Python_Projects\\PPE-Analysis\\archive\\updated_train_files.txt\n",
      "Updated file created successfully: F:\\Python_Projects\\PPE-Analysis\\archive\\updated_val_files.txt\n",
      "Используем конфигурацию датасета: F:\\Python_Projects\\PPE-Analysis\\archive\\sh17_kaggle.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from utils import update, train_and_validate_models, plot_results\n",
    "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
    "%env CLEARML_API_HOST=https://api.clear.ml\n",
    "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
    "%env CLEARML_API_ACCESS_KEY=7WEMZUE8PWLX9OB7E22LG461Y5Z7PU\n",
    "%env CLEARML_API_SECRET_KEY=VYzoBBjUkR_vVAHcswMTGygqd5NlKhaMxDIQpDyp9LlxGcBFmnN14Msl4w6Ulq3U2x0 #ClearML\n",
    "\n",
    "# Задаём базовые директории и файлы\n",
    "archive_dir = r\"F:\\Python_Projects\\PPE-Analysis\\archive\"\n",
    "images_root = os.path.join(archive_dir, \"images\")\n",
    "\n",
    "# Пути к исходным файлам с путями\n",
    "train_files = os.path.join(archive_dir, \"train_files.txt\")\n",
    "val_files = os.path.join(archive_dir, \"val_files.txt\")\n",
    "\n",
    "# Обновляем файлы с путями\n",
    "updated_train_files = update(train_files, images_root)\n",
    "updated_val_files = update(val_files, images_root)\n",
    "\n",
    "# Путь к конфигурационному файлу датасета\n",
    "data_config = os.path.join(archive_dir, \"sh17_kaggle.yaml\")\n",
    "print(f\"Используем конфигурацию датасета: {data_config}\")\n",
    "if not os.path.exists(data_config):\n",
    "    raise FileNotFoundError(f\"Конфигурационный файл не найден: {data_config}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142af9cb-4827-4ede-b049-c6b69a9c0476",
   "metadata": {},
   "source": [
    "### Определение моделей и запуск обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ecc0cbe-18cf-47c0-85d8-9e83349ac836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Обучение модели YOLOv11 ===\n",
      "Начало обучения YOLOv11 на датасете F:\\Python_Projects\\PPE-Analysis\\archive\\sh17_kaggle.yaml ...\n",
      "New https://pypi.org/project/ultralytics/8.3.92 available  Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=F:\\Python_Projects\\PPE-Analysis\\archive\\yolo11m.pt, data=F:\\Python_Projects\\PPE-Analysis\\archive\\sh17_kaggle.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cuda:0, workers=8, project=runs\\detect, name=train_YOLOv11, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train_YOLOv11\n",
      "Overriding model.yaml nc=80 with nc=17\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   1424131  ultralytics.nn.modules.head.Detect           [17, [256, 512, 512]]         \n",
      "YOLO11m summary: 231 layers, 20,066,115 parameters, 20,066,099 gradients, 68.3 GFLOPs\n",
      "\n",
      "Transferred 643/649 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train_YOLOv11', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning F:\\Python_Projects\\PPE-Analysis\\archive\\labels... 6479 images, 0 backgrounds, 0 corrupt: 100%|█████████\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: F:\\Python_Projects\\PPE-Analysis\\archive\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\Python_Projects\\PPE-Analysis\\archive\\labels... 1620 images, 0 backgrounds, 0 corrupt: 100%|██████████|\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: F:\\Python_Projects\\PPE-Analysis\\archive\\labels.cache\n",
      "Plotting labels to runs\\detect\\train_YOLOv11\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000476, momentum=0.9) with parameter groups 106 weight(decay=0.0), 113 weight(decay=0.0005), 112 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train_YOLOv11\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      8.36G      1.138      1.425      1.243        134        640: 100%|██████████| 405/405 [03:47<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 51/51 [00:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1620      15358      0.712      0.346      0.361       0.22\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      8.28G      1.196      1.077      1.266        218        640: 100%|██████████| 405/405 [03:38<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 51/51 [00:38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1620      15358       0.68      0.351      0.371      0.219\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      8.29G      1.202      1.052      1.274        143        640: 100%|██████████| 405/405 [03:46<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 51/51 [00:30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1620      15358      0.748      0.337      0.383      0.228\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      8.28G      1.168     0.9791      1.251        104        640: 100%|██████████| 405/405 [03:48<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 51/51 [00:33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1620      15358      0.671      0.364      0.394      0.242\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      10.8G      1.119     0.9069      1.225         91        640: 100%|██████████| 405/405 [04:39<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 51/51 [00:37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1620      15358       0.69      0.415      0.446      0.276\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      8.33G      1.066     0.8311       1.19        138        640: 100%|██████████| 405/405 [03:52<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 51/51 [00:36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1620      15358      0.542      0.467      0.468      0.292\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      8.31G      1.032      0.773      1.166        118        640: 100%|██████████| 405/405 [04:17<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 51/51 [01:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1620      15358      0.674      0.452      0.498       0.31\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10       8.1G     0.9892     0.7163      1.138        159        640: 100%|██████████| 405/405 [03:46<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 51/51 [00:30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1620      15358      0.601      0.507      0.518       0.33\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      10.6G     0.9501     0.6631      1.111         99        640: 100%|██████████| 405/405 [03:50<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 51/51 [00:31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1620      15358      0.632      0.531      0.551      0.353\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      10.6G     0.9127      0.619       1.09        128        640: 100%|██████████| 405/405 [03:52<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 51/51 [00:32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1620      15358      0.647       0.54      0.566      0.364\n",
      "\n",
      "10 epochs completed in 0.795 hours.\n",
      "Optimizer stripped from runs\\detect\\train_YOLOv11\\weights\\last.pt, 40.5MB\n",
      "Optimizer stripped from runs\\detect\\train_YOLOv11\\weights\\best.pt, 40.5MB\n",
      "\n",
      "Validating runs\\detect\\train_YOLOv11\\weights\\best.pt...\n",
      "Ultralytics 8.3.91  Python-3.10.11 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080, 12288MiB)\n",
      "YOLO11m summary (fused): 125 layers, 20,043,139 parameters, 0 gradients, 67.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 51/51 [00:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1620      15358      0.647       0.54      0.566      0.365\n",
      "                person       1515       2734      0.835      0.888       0.91      0.746\n",
      "                   ear        987       1612      0.853       0.72      0.763      0.485\n",
      "              ear-mufs         38         49      0.437      0.184      0.242      0.122\n",
      "                  face       1155       1855      0.924      0.868      0.908      0.677\n",
      "            face-guard         23         24      0.484      0.333      0.341      0.148\n",
      "             face-mask         75        151       0.72      0.649      0.673      0.402\n",
      "                  foot         64        149       0.51     0.0805      0.158     0.0693\n",
      "                  tool        455        923      0.533      0.265      0.302      0.165\n",
      "               glasses        323        398       0.65      0.601      0.605      0.319\n",
      "                gloves        254        529       0.62      0.539      0.569      0.356\n",
      "                helmet         93        154      0.695      0.656      0.677      0.465\n",
      "                 hands       1284       3212      0.822      0.825       0.86      0.597\n",
      "                  head       1314       2427      0.895      0.887      0.913      0.705\n",
      "          medical-suit         30         43       0.36      0.372      0.358      0.179\n",
      "                 shoes        320        956      0.658      0.576      0.614      0.358\n",
      "           safety-suit         28         45       0.48      0.226      0.251      0.134\n",
      "           safety-vest         45         97      0.521      0.515      0.474       0.27\n",
      "Speed: 0.3ms preprocess, 2.5ms inference, 0.0ms loss, 3.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train_YOLOv11\u001b[0m\n",
      "Обучение модели YOLOv11 завершено за 3040.71 секунд.\n",
      "Выполняется валидация модели YOLOv11...\n",
      "Ultralytics 8.3.91  Python-3.10.11 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080, 12288MiB)\n",
      "YOLO11m summary (fused): 125 layers, 20,043,139 parameters, 0 gradients, 67.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning F:\\Python_Projects\\PPE-Analysis\\archive\\labels.cache... 1620 images, 0 backgrounds, 0 corrupt: 100%|█████\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 102/102 [00:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1620      15358      0.643      0.542      0.566      0.364\n",
      "                person       1515       2734      0.834      0.888      0.911      0.746\n",
      "                   ear        987       1612      0.854      0.721      0.763      0.489\n",
      "              ear-mufs         38         49      0.435      0.184      0.241      0.121\n",
      "                  face       1155       1855      0.925       0.87      0.909      0.677\n",
      "            face-guard         23         24       0.48      0.333      0.341      0.148\n",
      "             face-mask         75        151      0.713      0.657      0.668      0.397\n",
      "                  foot         64        149      0.479     0.0805      0.155     0.0685\n",
      "                  tool        455        923      0.523      0.267      0.302      0.164\n",
      "               glasses        323        398      0.646      0.606      0.608      0.319\n",
      "                gloves        254        529      0.615      0.537      0.566      0.356\n",
      "                helmet         93        154      0.682      0.656       0.68      0.467\n",
      "                 hands       1284       3212       0.82      0.827      0.861      0.598\n",
      "                  head       1314       2427      0.891      0.887      0.913      0.706\n",
      "          medical-suit         30         43      0.364      0.372      0.348      0.169\n",
      "                 shoes        320        956      0.656      0.578      0.617       0.36\n",
      "           safety-suit         28         45       0.49      0.235      0.257      0.136\n",
      "           safety-vest         45         97      0.517      0.518      0.478      0.272\n",
      "Speed: 0.2ms preprocess, 4.4ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train_YOLOv112\u001b[0m\n",
      "Найден файл с результатами: runs\\detect\\train_YOLOv11\\results.csv\n",
      "Результаты YOLOv11: Precision=0.64743, Recall=0.54047, mAP50=0.56587, mAP50-95=0.36447\n",
      "\n",
      "=== Результаты сохранены в final_results.csv ===\n"
     ]
    }
   ],
   "source": [
    "# Словарь с вариантами моделей и путями к весовым файлам\n",
    "models_to_train = {\n",
    "    # \"YOLOv8n\": os.path.join(archive_dir, \"yolov8m.pt\"),\n",
    "    # \"YOLOv9\": os.path.join(archive_dir, \"yolov9m.pt\"),\n",
    "    # \"YOLOv10\": os.path.join(archive_dir, \"yolov10m.pt\"),\n",
    "    \"YOLOv11\": os.path.join(archive_dir, \"yolo11m.pt\"),\n",
    "    # \"YOLOv12\": os.path.join(archive_dir, \"yolo12m.pt\")\n",
    "}\n",
    "\n",
    "# Настройки для обучения\n",
    "project_name = r\"runs\\detect\"  # Папка для сохранения результатов\n",
    "epochs = 10\n",
    "\n",
    "# Запуск обучения и валидации моделей\n",
    "results_list = train_and_validate_models(models_to_train, data_config,\n",
    "                                         project_name, epochs)\n",
    "\n",
    "# Формирование DataFrame с результатами и сохранение в CSV\n",
    "results_df = pd.DataFrame(results_list)\n",
    "output_csv_path = \"final_results.csv\"\n",
    "results_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\n=== Результаты сохранены в {output_csv_path} ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4871ec30-0042-467a-9244-57245ea1f56d",
   "metadata": {},
   "source": [
    "### Визуализация результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7208db8-ad7f-4056-89b4-6b6e814622bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_results(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
